### Vulnerability List

#### 1. Configuration Parameter Manipulation for Skewed Dataset Generation

* Description:
    1. An attacker can control the `treatment_effect_fn` and `selection_bias_fn` parameters when initializing the `Credence` class.
    2. The attacker crafts a malicious function designed to introduce subtle biases into the generated datasets.
    3. This function is passed as an argument during the initialization of the `Credence` object.
    4. When the `Credence` framework generates datasets using the `sample()` method, the malicious function is executed as part of the data generation process.
    5. The generated datasets contain subtle biases that are not immediately apparent to users.
    6. Users unknowingly use these biased datasets to evaluate causal inference methods.
    7. The biases in the dataset skew the evaluation results, potentially leading users to select suboptimal or vulnerable causal inference methods.

* Impact:
    Users evaluating causal inference methods with datasets generated by CREDENCE may be misled. They might incorrectly assess the performance of different causal inference methods due to the subtle biases introduced through manipulated configuration parameters. This can lead to the selection of inappropriate or less effective methods for real-world causal inference problems, potentially impacting critical decisions based on these methods.

* Vulnerability Rank: High

* Currently Implemented Mitigations:
    None. The code directly accepts and uses user-provided functions for treatment effect and selection bias without any validation or sanitization.

* Missing Mitigations:
    - Input validation and sanitization for `treatment_effect_fn` and `selection_bias_fn` parameters in the `Credence` class constructor.
    - Implement predefined, safe options for treatment effect and selection bias functions within the framework, limiting user-defined functions or restricting them to a safe subset of operations.
    - Documentation highlighting the security risks of providing untrusted functions as configuration parameters.

* Preconditions:
    - The attacker needs to be able to specify the configuration parameters, specifically `treatment_effect_fn` and `selection_bias_fn`, when creating a `Credence` object.
    - This assumes the user can directly interact with the Python code or that a higher-level interface exposes these parameters without proper sanitization.

* Source Code Analysis:
    1. **`credence-v2/src/credence/__init__.py`:**
        - The `Credence` class `__init__` method accepts `treatment_effect_fn` and `selection_bias_fn` as parameters:
        ```python
        class Credence:
            def __init__(
                self,
                data,
                post_treatment_var,
                treatment_var,
                categorical_var,
                numerical_var,
                var_bounds={},
            ):
                # ...
                # ... generator for Y(1),Y(0) | X, T
                self.m_post = autoencoder.conVAE(
                    df=self.data_processed,
                    Xnames=self.Xnames + self.Tnames,
                    Ynames=self.Ynames,
                    cat_cols=self.categorical_var,
                    var_bounds=self.var_bounds,
                    latent_dim=latent_dim,
                    hidden_dim=hidden_dim,
                    potential_outcome=True,
                    treatment_cols=self.Tnames,
                    treatment_effect_fn=treatment_effect_fn, # User-provided function passed directly
                    selection_bias_fn=selection_bias_fn,     # User-provided function passed directly
                    effect_rigidity=effect_rigidity,
                    bias_rigidity=bias_rigidity,
                    kld_rigidity=kld_rigidity,
                )
                # ...
        ```
        - The user-provided `treatment_effect_fn` and `selection_bias_fn` are directly passed to the `conVAE` class constructor.

    2. **`credence-v2/src/credence/autoencoder.py`:**
        - The `conVAE` class `__init__` method stores these functions as attributes:
        ```python
        class conVAE(pl.LightningModule):
            def __init__(
                self,
                df,
                Ynames,
                Xnames=[],
                cat_cols=[],
                var_bounds={},
                latent_dim=2,
                hidden_dim=[16],
                batch_size=10,
                potential_outcome=False,
                treatment_cols=["T"],
                treatment_effect_fn=lambda x: 0, # Stored as self.f
                selection_bias_fn=lambda x, t: 0, # Stored as self.g
                effect_rigidity=1e20,
                bias_rigidity=1e20,
                kld_rigidity=0.1,
            ):
                super().__init__()
                # ...
                if self.potential_outcome:
                    # ...
                    self.f = treatment_effect_fn # Assignment of user-provided function
                    # ...
                    self.g = selection_bias_fn # Assignment of user-provided function
                # ...
        ```
        - The `loss_fn` method in `conVAE` directly calls these stored functions during loss calculation:
        ```python
        class conVAE(pl.LightningModule):
            # ...
            def loss_fn(self, yhat, y, mu, logvar, **kwargs):
                if self.potential_outcome:
                    # ...
                    # treatment effect constraint
                    constraint_effect = torch.sum(
                        torch.square(yhat[:, 1] - yhat[:, 0] - self.f(X)) # Direct call to user-provided function self.f
                    )

                    # selection bias constraint
                    constraint_bias = torch.sum(
                        torch.square(
                            T * (yhat[:, 0] - yhat_prime[:, 0])
                            + (1 - T) * (yhat_prime[:, 1] - yhat[:, 1])
                            - self.g(X, T) # Direct call to user-provided function self.g
                        )
                    )
                    # ...
        ```
        - There is no input validation or sanitization before calling `self.f(X)` and `self.g(X, T)`, allowing for potential arbitrary code execution or introduction of subtle biases if a malicious function is provided.

* Security Test Case:
    1. **Malicious Function Definition:** Define a malicious `treatment_effect_fn` in Python that introduces a bias and logs a message to confirm execution:
        ```python
        def malicious_treatment_effect(x):
            print("Malicious treatment effect function executed!")
            return 0.1 * torch.mean(x, dim=1) # Subtle bias depending on input X
        ```
    2. **Initialize Credence with Malicious Function:** In a test script, initialize the `Credence` class, passing `malicious_treatment_effect` as the `treatment_effect_fn`:
        ```python
        import pandas as pd
        import numpy as np
        import torch
        import credence

        # Toy dataset
        X = np.random.normal(0,1,(200,5))
        Y0 = np.random.normal(np.sum(X,axis=1),1)
        T = np.random.binomial(1,0.5,size=(X.shape[0],))
        df = pd.DataFrame(X,columns=['X%d'%(i) for i in range(X.shape[1])])
        Y1 = Y0**2 + np.random.normal(np.mean(X,axis=1),5)
        df['T'] = T
        df['Y'] = T*Y1 + (1-T)*Y0

        def malicious_treatment_effect(x):
            print("Malicious treatment effect function executed!")
            return 0.1 * torch.mean(x, dim=1)

        v_malicious = credence.Credence(
            data=df,
            post_treatment_var=['Y'],
            treatment_var=['T'],
            categorical_var=['T'],
            numerical_var=['X%d'%(i) for i in range(X.shape[1])]+['Y'],
            treatment_effect_fn=malicious_treatment_effect # Injecting malicious function
        )
        ```
    3. **Train and Sample with Malicious Configuration:** Train the `Credence` model and generate a dataset:
        ```python
        gen_malicious = v_malicious.fit(effect_rigidity=0, max_epochs=1) # Short epoch for testing
        df_gen_malicious = v_malicious.sample()
        ```
    4. **Verify Malicious Function Execution and Bias:** Check the console output to confirm "Malicious treatment effect function executed!" is printed, indicating the malicious function was indeed called. Analyze the generated dataset (`df_gen_malicious`) for subtle biases in treatment effects compared to a dataset generated with a benign `treatment_effect_fn` (e.g., `lambda x: 0`). This bias can be observed by comparing the average treatment effect across datasets generated with both functions.
    5. **Evaluate Causal Inference Methods:** Evaluate a set of causal inference methods on both the dataset generated with the malicious function and a dataset generated with a benign function. Compare the performance metrics and rankings of the methods. Demonstrate that the malicious bias skews the results, potentially favoring certain methods over others in the maliciously generated dataset.

This test case will demonstrate that a user-provided, potentially malicious function for `treatment_effect_fn` can be executed by the CREDENCE framework and can subtly bias the generated datasets, leading to skewed evaluation results for causal inference methods.