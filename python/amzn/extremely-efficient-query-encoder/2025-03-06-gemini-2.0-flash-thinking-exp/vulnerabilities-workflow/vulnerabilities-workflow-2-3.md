- Vulnerability Name: Data Poisoning in Training Data
- Description:
    1. The project trains query encoders using training data loaded from files or datasets, as specified in configuration files and command-line arguments in scripts like `run_pretraining.pretrain`, `marco_train_pretrained_model.sh`, and `tevatron.driver.train`. Example data paths include `../resources/pretrain_data/train_queries_tokens.jsonl`, `nq-train/bm25.bert.json`, and `marco/bert/train/split*.json`.
    2. An attacker can compromise the integrity of the query encoder by manipulating these training data files before the training process starts. This manipulation involves injecting malicious data into the training set.
    3. The injected malicious data can consist of crafted query-passage pairs designed to mislead the model during training. This could include associating benign queries with irrelevant or biased passages, or associating attacker-controlled queries with highly relevant but potentially harmful passages.
    4. When the training process is executed, the query encoder learns from the poisoned dataset. The learning algorithms in `tevatron.driver.train` and related scripts do not include any mechanisms to detect or filter out data poisoning attacks.
    5. As a result of training on poisoned data, the query encoder's performance can be severely compromised. This can manifest as degraded retrieval accuracy, introduction of biases that skew search results, or optimization of the model to favor queries controlled by the attacker.
- Impact:
    - **Degraded Retrieval Performance**: Legitimate users will experience a noticeable decline in the quality of search results. Relevant documents may be missed, and irrelevant or harmful documents might be retrieved instead.
    - **Bias Introduction**: The query encoder can become biased, leading to unfair or discriminatory search outcomes. This is particularly concerning in applications where fairness and neutrality are critical.
    - **Compromised Downstream Applications**: If the poisoned query encoder is used in a downstream application (e.g., a search engine, recommendation system), the application's reliability and trustworthiness will be undermined, potentially causing reputational damage and user dissatisfaction.
- Vulnerability Rank: High
- Currently Implemented Mitigations: None. The project's current implementation lacks any built-in mechanisms to validate or ensure the integrity of the training data. It implicitly trusts that the input data is clean and free from malicious modifications.
- Missing Mitigations:
    - **Input Data Validation and Sanitization**: Implement robust validation checks on all input training data. This should include schema validation to ensure data conforms to expected formats and content validation to detect anomalies or suspicious patterns in query-passage pairs.
    - **Data Provenance and Integrity Checks**: Establish a system for tracking the origin and history of training data. Employ cryptographic techniques such as hashing or digital signatures to verify the integrity of data files and detect unauthorized alterations.
    - **Anomaly Detection in Training Data**: Integrate anomaly detection algorithms to automatically identify and flag unusual or suspicious data points within the training dataset. This can help in preemptively filtering out potentially poisoned samples before they affect model training.
    - **Robust Training Techniques**: Explore and implement training methodologies that are inherently more resilient to data poisoning attacks. This could involve techniques like robust optimization, which aims to minimize the influence of outliers, or outlier-aware loss functions that reduce the impact of noisy or malicious data points.
- Preconditions:
    - The attacker needs to gain the ability to modify or replace the training data files that are used by the project's training scripts. This could be achieved through various means, such as:
        - Compromising the storage location of the training data (e.g., a file server, cloud storage bucket).
        - Intercepting or manipulating the data stream if the training data is fetched from an external source over a network.
        - Supply chain attacks, where a dependency or data source used by the project is compromised.
        - Insider threats, where a malicious user with authorized access to the data modifies it.
- Source Code Analysis:
    - **Data Loading Process**: The project relies on datasets loaded using file paths that are often hardcoded or specified via configuration files. For example, the `run_pretraining.pretrain.py` script uses `PRETRAIN_DATA_PATH = "../resources/pretrain_data"` and loads data based on filenames within this path. Similarly, example scripts for training models on MS MARCO and NQ datasets reference data files in local directories (e.g., `./marco/bert/train`, `nq-train`).
    - **Absence of Integrity Checks**: A review of the provided Python scripts (`tevatron/driver/train.py`, `run_pretraining/pretrain.py`, dataset loading scripts in `tevatron/datasets/`) reveals a complete lack of input validation or data integrity checks. The code directly reads and processes data from the specified file paths without any verification of its source, format, or content.
    - **External Data Sources**: Scripts like `examples/coCondenser-marco/get_data.sh` demonstrate the project's reliance on external data sources, downloading datasets from URLs (e.g., `https://rocketqa.bj.bcebos.com/corpus/marco.tar.gz`, `https://msmarco.blob.core.windows.net/msmarcoranking/qidpidtriples.train.full.2.tsv.gz`). These download processes are potential points of failure if the download sources are compromised or if man-in-the-middle attacks are possible. The scripts use `wget` without strong integrity checks on downloaded files beyond basic checks like `tar -zxf marco.tar.gz` which does not verify the content's origin or prevent sophisticated poisoning.
- Security Test Case:
    1. **Setup**:
        - Set up a development environment with the project code and necessary dependencies.
        - Choose a training example to target, such as pre-training using `run_pretraining.pretrain` and identify the relevant training data input file, for instance, `../resources/pretrain_data/train_queries_tokens.jsonl`.
        - Train a baseline model using the original, clean training data and record its performance metrics (e.g., MRR@10, NDCG@10) on a held-out evaluation dataset. This serves as the benchmark for comparison.
    2. **Data Poisoning**:
        - Introduce poisoned data into the training data file (`../resources/pretrain_data/train_queries_tokens.jsonl`). For example, for every 200 lines, insert one poisoned data entry. A poisoned entry could be crafted to misguide the model. For instance, alter the 'target' vector in a few entries to represent embeddings that are significantly different from what a clean model would produce for those queries, or introduce queries that are semantically unrelated to their target vectors.
    3. **Retraining with Poisoned Data**:
        - Execute the pre-training script (`python -m run_pretraining.pretrain`) using the modified, poisoned training data. This will train a new query encoder model that has been exposed to the injected malicious data.
    4. **Evaluation and Comparison**:
        - Encode a set of clean evaluation queries and passages using both the baseline model (trained on clean data) and the poisoned model.
        - Evaluate the retrieval performance of both models on the same evaluation dataset. Use standard information retrieval metrics like MRR@10 and NDCG@10 to quantify the performance.
        - Compare the evaluation metrics of the poisoned model against the baseline model. A successful data poisoning attack will manifest as a measurable degradation in the poisoned model's retrieval performance compared to the baseline. Additionally, you might observe biased behavior, such as the poisoned model retrieving irrelevant documents for certain queries or showing a preference for documents related to the attacker's chosen bias.
    5. **Verification of Vulnerability**:
        - If the poisoned model demonstrates a statistically significant decrease in retrieval performance metrics or exhibits noticeable bias in retrieval results compared to the baseline model, this confirms the presence of a data poisoning vulnerability. The magnitude of performance degradation and the nature of the bias will indicate the severity and type of poisoning attack that is possible.