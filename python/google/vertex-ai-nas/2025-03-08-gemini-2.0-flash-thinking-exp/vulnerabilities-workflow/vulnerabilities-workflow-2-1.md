- Vulnerability Name: Insecure Handling of Training Data Paths in Tutorials
- Description:
  1. The tutorials, specifically `third_party/tutorial/vertex_tutorial1.md` and `third_party/tutorial/vertex_tutorial4.md`, instruct users to pass training data paths as command-line arguments to the `vertex_nas_cli.py` script.
  2. For example, in `third_party/tutorial/vertex_tutorial1.md`, the training command includes flags like `--local_data_dir` and `--root_output_dir` which can be GCS paths. Similarly, `third_party/tutorial/vertex_tutorial4.md` uses `--root_output_dir` for cloud training jobs.
  3. These paths are directly used by the NAS code, including trainer dockers, to access training data.
  4. An attacker could potentially manipulate these publicly available tutorial instructions to inject malicious GCS paths, pointing to attacker-controlled datasets.
  5. If a user, following the tutorial, inadvertently uses a malicious data path provided by an attacker (e.g., through a modified tutorial or forum post), the NAS training process will use this poisoned data.
  6. This could lead to data poisoning attacks where the NAS service trains models on attacker-controlled data, resulting in backdoors or biases in the generated models.
- Impact:
  - Data Poisoning: An attacker can influence the training process by providing malicious training data, leading to compromised models with backdoors or biases.
  - Compromised Model Integrity: Models generated by Vertex AI NAS may be unreliable and insecure due to training on poisoned data.
  - Reputational Damage: Google Vertex AI NAS reputation could be damaged if users unknowingly deploy vulnerable models trained using these tutorials and are subsequently attacked.
- Vulnerability Rank: High
- Currently Implemented Mitigations:
  - None: The tutorials directly instruct users to provide GCS paths without explicit warnings about data source verification or sanitization. The code itself within the provided snippets doesn't implement input validation for these paths in the context of the tutorials.
- Missing Mitigations:
  - Security Warnings: Add prominent security warnings in the tutorials, explicitly advising users to:
    - Carefully verify the source and integrity of any training data, especially when using externally provided instructions or configurations.
    - Use data from trusted and controlled GCS buckets.
    - Be cautious about executing commands from untrusted sources.
  - Documentation on Data Poisoning Risks: Include a dedicated section in the documentation explaining the risks of data poisoning in NAS and how users can mitigate these risks.
- Preconditions:
  - User follows publicly available tutorials.
  - User inadvertently uses a malicious data path provided by an attacker through a modified tutorial or other means.
  - User has a Google Cloud project set up and is using Vertex AI NAS.
- Source Code Analysis:
  1. File: `/code/third_party/tutorial/vertex_tutorial1.md` and `/code/third_party/tutorial/vertex_tutorial4.md`
  2. These files contain instructions that guide users to run `vertex_nas_cli.py` with flags like `--local_data_dir`, `--root_output_dir`, and `--search_docker_flags` which can include data paths.
  3. For example, `vertex_tutorial1.md` shows commands like:
     ```sh
     python3 vertex_nas_cli.py search_in_local ... --local_data_dir=${DATA_DIR} ...
     python3 vertex_nas_cli.py search ... --root_output_dir=${GCS_ROOT_DIR} ...
     ```
  4. `vertex_tutorial4.md` also demonstrates similar commands using GCS paths.
  5. The `vertex_nas_cli.py` script (File: `/code/vertex_nas_cli.py`) processes these command-line arguments and passes them to the training jobs.
  6. The trainer code (e.g., `tutorial/tutorial1_mnist_search.py`) uses these paths to access data, as shown in the tutorial steps.
  7. There is no explicit input validation or sanitization of these paths in the provided tutorial code or `vertex_nas_cli.py` for the tutorial use cases.
- Security Test Case:
  1. Setup:
     - Create a benign Google Cloud project and set up Vertex AI NAS according to the tutorials.
     - Create a malicious GCS bucket (`gs://malicious-data-bucket`) controlled by the attacker.
     - In the malicious bucket, create a "poisoned" training dataset designed to introduce a backdoor into the trained model.
  2. Attacker Action:
     - Modify the tutorial instructions (e.g., on a public forum or a cloned repository) to replace the example GCS data paths with the attacker's malicious GCS path (`gs://malicious-data-bucket`).
     - For example, modify the `GCS_ROOT_DIR` in `vertex_tutorial1.md` to point to `gs://malicious-data-bucket`.
  3. User Action:
     - A user, following the modified tutorial, copies and pastes the commands, inadvertently using the attacker's malicious GCS path.
     - User runs the NAS search or training job using `vertex_nas_cli.py` with the malicious data path.
  4. Expected Outcome (Vulnerability Confirmation):
     - The Vertex AI NAS training job uses the data from `gs://malicious-data-bucket` for training.
     - The generated model (if training completes) exhibits backdoor behavior or biases as a result of the data poisoning attack.
     - The user's Vertex AI NAS project is now at risk of deploying compromised models.